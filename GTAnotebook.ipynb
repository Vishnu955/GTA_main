{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "class ProbMask():\n",
    "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
    "        _mask = torch.ones(L, scores.shape[-1], dytpe=torch.bool).to(device).triu(1)\n",
    "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
    "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
    "                             torch.arange(H)[None, :, None],\n",
    "                             index, :].to(device)\n",
    "        self._mask = indicator.view(scores.shape).to(device)\n",
    "    \n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1./sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        return V.contiguous()\n",
    "\n",
    "class ProbAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
    "        # Q [B, H, L, D]\n",
    "        B, H, L, E = K.shape\n",
    "        _, _, S, _ = Q.shape\n",
    "\n",
    "        # calculate the sampled Q_K\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, S, L, E)\n",
    "        indx_sample = torch.randint(L, (S, sample_k))\n",
    "        K_sample = K_expand[:, :, torch.arange(S).unsqueeze(1), indx_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\n",
    "\n",
    "        # find the Top_k query with sparisty measurement\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        # use the reduced Q to calculate Q_K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                     torch.arange(H)[None, :, None],\n",
    "                     M_top, :]\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))\n",
    "\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            V_sum = V.sum(dim=-2)\n",
    "            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
    "        else: # use mask\n",
    "            assert(L_Q == L_V) # requires that L_Q == L_V, i.e. for self-attention only\n",
    "            contex = V.cumsum(dim=-1)\n",
    "        return contex\n",
    "\n",
    "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
    "        B, H, L_V, D = V.shape\n",
    "\n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1) # nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   index, :] = torch.matmul(attn, V)\n",
    "        return context_in\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, D = queries.shape\n",
    "        _, S, _, _ = keys.shape\n",
    "\n",
    "        queries = queries.view(B, H, L, -1)\n",
    "        keys = keys.view(B, H, S, -1)\n",
    "        values = values.view(B, H, S, -1)\n",
    "\n",
    "        U = self.factor * np.ceil(np.log(S)).astype('int').item()\n",
    "        u = self.factor * np.ceil(np.log(L)).astype('int').item()\n",
    "        \n",
    "        scores_top, index = self._prob_QK(queries, keys, u, U)\n",
    "        # add scale factor\n",
    "        scale = self.scale or 1./sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        # get the context\n",
    "        context = self._get_initial_context(values, L)\n",
    "        # update the context with selected top_k queries\n",
    "        context = self._update_context(context, values, scores_top, index, L, attn_mask)\n",
    "        \n",
    "        return context.contiguous()\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model//n_heads)\n",
    "        d_values = d_values or (d_model//n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        ).view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff =  4*d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask\n",
    "        ))\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask\n",
    "        ))\n",
    "\n",
    "        y = x = self.norm2(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1,1))\n",
    "\n",
    "        return self.norm3(x+y)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers, norm_layer=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.downConv = nn.Conv1d(in_channels=c_in,\n",
    "                                  out_channels=c_in,\n",
    "                                  kernel_size=3,\n",
    "                                  padding=2,\n",
    "                                  padding_mode='circular')\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff =  4*d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        x = x + self.dropout(self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask = attn_mask\n",
    "        ))\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1,1))\n",
    "\n",
    "        return self.norm2(x+y)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "            x = self.attn_layers[-1](x)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x = attn_layer(x, attn_mask=attn_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, \n",
    "                                    kernel_size=3, padding=padding, padding_mode='circular')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        return x\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', data='ETTh'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4; hour_size = 24\n",
    "        weekday_size = 7; day_size = 32; month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type=='fixed' else nn.Embedding\n",
    "        if data == 'ETTm':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        elif data == 'SolarEnergy':\n",
    "            minute_size = 6\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        elif data == 'WADI':\n",
    "            minute_size = 60\n",
    "            second_size = 6\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "            self.second_emebd = Embed(second_size, d_model)\n",
    "        elif data == 'SMAP':\n",
    "            minute_size = 60\n",
    "            second_size = 15\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "            self.second_emebd = Embed(second_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        \n",
    "        second_x = self.second_emebd(x[:,:,5]) if hasattr(self, 'second_embed') else 0.\n",
    "        minute_x = self.minute_embed(x[:,:,4]) if hasattr(self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:,:,3])\n",
    "        weekday_x = self.weekday_embed(x[:,:,2])\n",
    "        day_x = self.day_embed(x[:,:,1])\n",
    "        month_x = self.month_embed(x[:,:,0])\n",
    "        \n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x + second_x\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', data='ETTh', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        # self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, data=data)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        # x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        \n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# class Chomp1d(nn.Module):\n",
    "#     def __init__(self, chomp_size):\n",
    "#         super(Chomp1d, self).__init__()\n",
    "#         self.chomp_size = chomp_size\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size=3, stride=1, dilation=1, padding=1, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation,\n",
    "                                           padding_mode='circular'))\n",
    "        # self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation,\n",
    "                                           padding_mode='circular'))\n",
    "        # self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # self.conv1.weight.data.normal_(0, 0.01)\n",
    "        # self.conv2.weight.data.normal_(0, 0.01)\n",
    "        # if self.downsample is not None:\n",
    "        #     self.downsample.weight.data.normal_(0, 0.01)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size // 2, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTA model creation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential as Seq, Linear, ReLU, Parameter\n",
    "from torch_geometric.nn import MessagePassing, GCNConv\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AdaGCNConv(MessagePassing):\n",
    "    def __init__(self, num_nodes, in_channels, out_channels, improved=False, \n",
    "                    add_self_loops=False, normalize=True, bias=True, init_method='all'):\n",
    "        super(AdaGCNConv, self).__init__(aggr='add', node_dim=0) #  \"Max\" aggregation.\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.normalize = normalize\n",
    "        self.bias = bias\n",
    "        self.init_method = init_method\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self._init_graph_logits_()\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def _init_graph_logits_(self):\n",
    "        if self.init_method == 'all':\n",
    "            logits = .8 * torch.ones(self.num_nodes ** 2, 2)\n",
    "            logits[:, 1] = 0\n",
    "        elif self.init_method == 'random':\n",
    "            logits = 1e-3 * torch.randn(self.num_nodes ** 2, 2)\n",
    "        elif self.init_method == 'equal':\n",
    "            logits = .5 * torch.ones(self.num_nodes ** 2, 2)\n",
    "        else:\n",
    "            raise NotImplementedError('Initial Method %s is not implemented' % self.init_method)\n",
    "        \n",
    "        self.register_parameter('logits', Parameter(logits, requires_grad=True))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "        if self.normalize:\n",
    "            edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                            edge_index, edge_weight, x.size(self.node_dim),\n",
    "                            self.improved, self.add_self_loops, dtype=x.dtype)\n",
    "\n",
    "        z = torch.nn.functional.gumbel_softmax(self.logits, hard=True)\n",
    "        \n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
    "                             size=None, z=z)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, edge_weight, z):\n",
    "        if edge_weight is None:\n",
    "            return x_j * z[:, 0].contiguous().view([-1] + [1] * (x_j.dim() - 1))\n",
    "        else:\n",
    "            return edge_weight.view([-1] + [1] * (x_j.dim() - 1)) * x_j * z[:, 0].contiguous().view([-1] + [1] * (x_j.dim() - 1))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "\n",
    "\n",
    "class GraphTemporalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, seq_len, num_levels, kernel_size=3, dropout=0.02, device=torch.device('cuda:0')):\n",
    "        super(GraphTemporalEmbedding, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.seq_len = seq_len\n",
    "        self.num_levels = num_levels\n",
    "        self.device = device\n",
    "        assert (kernel_size - 1) // 2\n",
    "\n",
    "        self.tc_modules = torch.nn.ModuleList([])\n",
    "        self.gc_module = AdaGCNConv(num_nodes, seq_len, seq_len)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            self.tc_modules.extend([TemporalBlock(num_nodes, num_nodes, kernel_size=kernel_size, stride=1, dilation=dilation_size,\n",
    "                                        padding=(kernel_size-1) * dilation_size // 2, dropout=dropout)])\n",
    "            # self.gc_modules.extend([AdaGCNConv(num_nodes, seq_len, seq_len)])\n",
    "        \n",
    "        source_nodes, target_nodes = [], []\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(num_nodes):\n",
    "                source_nodes.append(j)\n",
    "                target_nodes.append(i)\n",
    "        self.edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long, device=\"cpu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # >> (bsz, seq_len, num_nodes)\n",
    "        x = x.permute(0, 2, 1) # >> (bsz, num_nodes, seq_len)\n",
    "\n",
    "        x = self.tc_modules[0](x) # >> (bsz, num_nodes, seq_len)\n",
    "        x = self.gc_modules[0](x.transpose(0, 1), self.edge_index).transpose(0, 1) # >> (bsz, num_nodes, seq_len)\n",
    "        # output = x\n",
    "        \n",
    "        for i in range(1, self.num_levels):\n",
    "            x = self.tc_modules[i](x) # >> (bsz, num_nodes, seq_len)\n",
    "            x = self.gc_module(x.transpose(0, 1), self.edge_index).transpose(0, 1) # >> (bsz, num_nodes, seq_len)\n",
    "            # output += x\n",
    "\n",
    "        # return output.transpose(1, 2) # >> (bsz, seq_len, num_nodes)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "\n",
    "class generateGTA(nn.Module):\n",
    "    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len, \n",
    "                factor=5, d_model=512, n_heads=8, e_layers=3, d_layers=2, d_ff=512, \n",
    "                dropout=0.0, attn='prob', embed='fixed', data='ETTh', activation='gelu', # need to change data to our data\n",
    "                device=torch.device('cuda:0')):\n",
    "        super(generateGTA, self).__init__()\n",
    "        self.pred_len = out_len\n",
    "        self.attn = attn\n",
    "\n",
    "        # Encoding\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, data, dropout)\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, data, dropout)\n",
    "        # Attention\n",
    "        Attn = ProbAttention if attn=='prob' else FullAttention\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(Attn(False, factor, attention_dropout=dropout), \n",
    "                                d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            [\n",
    "                ConvLayer(\n",
    "                    d_model\n",
    "                ) for l in range(e_layers-1)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(FullAttention(True, factor, attention_dropout=dropout), \n",
    "                                d_model, n_heads),\n",
    "                    AttentionLayer(FullAttention(False, factor, attention_dropout=dropout), \n",
    "                                d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                )\n",
    "                for l in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        # self.end_conv1 = nn.Conv1d(in_channels=label_len+out_len, out_channels=out_len, kernel_size=1, bias=True)\n",
    "        # self.end_conv2 = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=1, bias=True)\n",
    "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
    "        \n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, \n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "        dec_out = self.projection(dec_out)\n",
    "        \n",
    "        return dec_out[:,-self.pred_len:,:] \n",
    "\n",
    "\n",
    "\n",
    "class GTA(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, seq_len, label_len, out_len, num_levels,\n",
    "                factor=5, d_model=512, n_heads=8, e_layers=3, d_layers=2, d_ff=512, \n",
    "                dropout=0.0, attn='prob', embed='fixed', data='ETTh', activation='gelu', #change to our data \n",
    "                device=torch.device('cuda:0')):\n",
    "        super(GTA, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.out_len = out_len\n",
    "        self.num_levels = num_levels\n",
    "        self.device = device\n",
    "\n",
    "        self.gt_embedding = GraphTemporalEmbedding(num_nodes, seq_len, num_levels, kernel_size=3, dropout=dropout, device=device)\n",
    "        self.model = generateGTA(num_nodes, num_nodes, num_nodes, seq_len, label_len, out_len, \n",
    "                            factor, d_model, n_heads, e_layers, d_layers, d_ff, \n",
    "                            dropout, attn, embed, data, activation, device)\n",
    "    \n",
    "    def forward(self, batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "        batch_x = self.gt_embedding(batch_x) # >> (bsz, seq, num_nodes)\n",
    "\n",
    "\n",
    "        # removed the below line cause transepose is not required need to check with our data once \n",
    "\n",
    "        \n",
    "        # batch_x = batch_x.transpose(0, 1).transpose(1, 2) # >> (bsz, seq_len, num_nodes)\n",
    "        dec_inp = torch.zeros_like(batch_y[:,-self.out_len:,:]).double().to(self.device)\n",
    "        dec_inp = torch.cat([batch_y[:,:self.label_len,:], dec_inp], dim=1).double().to(self.device)\n",
    "        output = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exp_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Exp_Basic(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.device = self._acquire_device()\n",
    "        self.model = self._build_model().to(\"cpu\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        raise NotImplementedError\n",
    "        return None\n",
    "    \n",
    "    def _acquire_device(self): #need to see how to change for my system it doesn't have cuda drivers \n",
    "        if self.args.use_gpu:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(self.args.gpu)\n",
    "            device = torch.device('cuda:0')\n",
    "            print('Use GPU: cuda:0')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print('Use CPU')\n",
    "        return device\n",
    "\n",
    "    def _get_data(self):\n",
    "        pass\n",
    "\n",
    "    def vali(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def test(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Utlils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Utlils\u001b[39m.\u001b[39mmetrics\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Utlils' is not defined"
     ]
    }
   ],
   "source": [
    "Utlils.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def RSE(pred, true):\n",
    "    return np.sqrt(np.sum((true-pred)**2)) / np.sqrt(np.sum((true-true.mean())**2))\n",
    "\n",
    "def CORR(pred, true):\n",
    "    u = ((true-true.mean(0))*(pred-pred.mean(0))).sum(0) \n",
    "    d = np.sqrt(((true-true.mean(0))**2*(pred-pred.mean(0))**2).sum(0))\n",
    "    return (u/d).mean(-1)\n",
    "\n",
    "def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred-true))\n",
    "\n",
    "def MSE(pred, true):\n",
    "    return np.mean((pred-true)**2)\n",
    "\n",
    "def RMSE(pred, true):\n",
    "    return np.sqrt(MSE(pred, true))\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    return np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "def MSPE(pred, true):\n",
    "    return np.mean(np.square((pred - true) / true))\n",
    "\n",
    "def metric(pred, true):\n",
    "    mae = MAE(pred, true)\n",
    "    mse = MSE(pred, true)\n",
    "    rmse = RMSE(pred, true)\n",
    "    mape = MAPE(pred, true)\n",
    "    mspe = MSPE(pred, true)\n",
    "    \n",
    "    return mae,mse,rmse,mape,mspe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils.spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from math import log,floor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# colors for plot\n",
    "deep_saffron = '#FF9933'\n",
    "air_force_blue = '#5D8AA8'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "================================= MAIN CLASS ==================================\n",
    "\"\"\"\n",
    "\n",
    "class SPOT:\n",
    "    \"\"\"\n",
    "    This class allows to run SPOT algorithm on univariate dataset (upper-bound)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    proba : float\n",
    "        Detection level (risk), chosen by the user\n",
    "        \n",
    "    extreme_quantile : float\n",
    "        current threshold (bound between normal and abnormal events)\n",
    "        \n",
    "    data : numpy.array\n",
    "        stream\n",
    "    \n",
    "    init_data : numpy.array\n",
    "        initial batch of observations (for the calibration/initialization step)\n",
    "    \n",
    "    init_threshold : float\n",
    "        initial threshold computed during the calibration step\n",
    "    \n",
    "    peaks : numpy.array\n",
    "        array of peaks (excesses above the initial threshold)\n",
    "    \n",
    "    n : int\n",
    "        number of observed values\n",
    "    \n",
    "    Nt : int\n",
    "        number of observed peaks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, q = 1e-4):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\t    Parameters\n",
    "\t    ----------\n",
    "\t    q\n",
    "\t\t    Detection level (risk)\n",
    "\t\n",
    "\t    Returns\n",
    "\t    ----------\n",
    "    \tSPOT object\n",
    "        \"\"\"\n",
    "        self.proba = q\n",
    "        self.extreme_quantile = None\n",
    "        self.data = None\n",
    "        self.init_data = None\n",
    "        self.init_threshold = None\n",
    "        self.peaks = None\n",
    "        self.n = 0\n",
    "        self.Nt = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        s += 'Streaming Peaks-Over-Threshold Object\\n'\n",
    "        s += 'Detection level q = %s\\n' % self.proba\n",
    "        if self.data is not None:\n",
    "            s += 'Data imported : Yes\\n'\n",
    "            s += '\\t initialization  : %s values\\n' % self.init_data.size\n",
    "            s += '\\t stream : %s values\\n' % self.data.size\n",
    "        else:\n",
    "            s += 'Data imported : No\\n'\n",
    "            return s\n",
    "            \n",
    "        if self.n == 0:\n",
    "            s += 'Algorithm initialized : No\\n'\n",
    "        else:\n",
    "            s += 'Algorithm initialized : Yes\\n'\n",
    "            s += '\\t initial threshold : %s\\n' % self.init_threshold\n",
    "            \n",
    "            r = self.n-self.init_data.size\n",
    "            if r > 0:\n",
    "                s += 'Algorithm run : Yes\\n'\n",
    "                s += '\\t number of observations : %s (%.2f %%)\\n' % (r,100*r/self.n)\n",
    "            else:\n",
    "                s += '\\t number of peaks  : %s\\n' % self.Nt\n",
    "                s += '\\t extreme quantile : %s\\n' % self.extreme_quantile\n",
    "                s += 'Algorithm run : No\\n'\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def fit(self,init_data,data):\n",
    "        \"\"\"\n",
    "        Import data to SPOT object\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    init_data : list, numpy.array or pandas.Series\n",
    "\t\t    initial batch to calibrate the algorithm\n",
    "            \n",
    "        data : numpy.array\n",
    "\t\t    data for the run (list, np.array or pd.series)\n",
    "\t\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            self.data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            self.data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            self.data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "            \n",
    "        if isinstance(init_data,list):\n",
    "            self.init_data = np.array(init_data)\n",
    "        elif isinstance(init_data,np.ndarray):\n",
    "            self.init_data = init_data\n",
    "        elif isinstance(init_data,pd.Series):\n",
    "            self.init_data = init_data.values\n",
    "        elif isinstance(init_data,int):\n",
    "            self.init_data = self.data[:init_data]\n",
    "            self.data = self.data[init_data:]\n",
    "        elif isinstance(init_data,float) & (init_data<1) & (init_data>0):\n",
    "            r = int(init_data*data.size)\n",
    "            self.init_data = self.data[:r]\n",
    "            self.data = self.data[r:]\n",
    "        else:\n",
    "            print('The initial data cannot be set')\n",
    "            return\n",
    "        \n",
    "    def add(self,data):\n",
    "        \"\"\"\n",
    "        This function allows to append data to the already fitted data\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    data : list, numpy.array, pandas.Series\n",
    "\t\t    data to append\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "        \n",
    "        self.data = np.append(self.data,data)\n",
    "        return\n",
    "    \n",
    "    def initialize(self, level = 0.98, verbose = True):\n",
    "        \"\"\"\n",
    "        Run the calibration (initialization) step\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "        level : float\n",
    "            (default 0.98) Probability associated with the initial threshold t \n",
    "\t    verbose : bool\n",
    "\t\t    (default = True) If True, gives details about the batch initialization\n",
    "        \"\"\"\n",
    "        level = level-floor(level)\n",
    "        \n",
    "        n_init = self.init_data.size\n",
    "        \n",
    "        S = np.sort(self.init_data)     # we sort X to get the empirical quantile\n",
    "        self.init_threshold = S[int(level*n_init)] # t is fixed for the whole algorithm\n",
    "\n",
    "        # initial peaks\n",
    "        self.peaks = self.init_data[self.init_data>self.init_threshold]-self.init_threshold \n",
    "        self.Nt = self.peaks.size\n",
    "        self.n = n_init\n",
    "        \n",
    "        if verbose:\n",
    "            print('Initial threshold : %s' % self.init_threshold)\n",
    "            print('Number of peaks : %s' % self.Nt)\n",
    "            print('Grimshaw maximum log-likelihood estimation ... ', end = '')\n",
    "            \n",
    "        g,s,l = self._grimshaw()\n",
    "        self.extreme_quantile = self._quantile(g,s)\n",
    "        \n",
    "        if verbose:\n",
    "            print('[done]')\n",
    "            print('\\t'+chr(0x03B3) + ' = ' + str(g))\n",
    "            print('\\t'+chr(0x03C3) + ' = ' + str(s))\n",
    "            print('\\tL = ' + str(l))\n",
    "            print('Extreme quantile (probability = %s): %s' % (self.proba,self.extreme_quantile))\n",
    "        \n",
    "        return \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _rootsFinder(fun,jac,bounds,npoints,method):\n",
    "        \"\"\"\n",
    "        Find possible roots of a scalar function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fun : function\n",
    "\t\t    scalar function \n",
    "        jac : function\n",
    "            first order derivative of the function  \n",
    "        bounds : tuple\n",
    "            (min,max) interval for the roots search    \n",
    "        npoints : int\n",
    "            maximum number of roots to output      \n",
    "        method : str\n",
    "            'regular' : regular sample of the search interval, 'random' : uniform (distribution) sample of the search interval\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        numpy.array\n",
    "            possible roots of the function\n",
    "        \"\"\"\n",
    "        if method == 'regular':\n",
    "            step = (bounds[1]-bounds[0])/(npoints+1)\n",
    "            X0 = np.arange(bounds[0]+step,bounds[1],step)\n",
    "        elif method == 'random':\n",
    "            X0 = np.random.uniform(bounds[0],bounds[1],npoints)\n",
    "        \n",
    "        def objFun(X,f,jac):\n",
    "            g = 0\n",
    "            j = np.zeros(X.shape)\n",
    "            i = 0\n",
    "            for x in X:\n",
    "                fx = f(x)\n",
    "                g = g+fx**2\n",
    "                j[i] = 2*fx*jac(x)\n",
    "                i = i+1\n",
    "            return g,j\n",
    "        \n",
    "        opt = minimize(lambda X:objFun(X,fun,jac), X0, \n",
    "                       method='L-BFGS-B', \n",
    "                       jac=True, bounds=[bounds]*len(X0))\n",
    "        \n",
    "        X = opt.x\n",
    "        np.round(X,decimals = 5)\n",
    "        return np.unique(X)\n",
    "    \n",
    "    \n",
    "    def _log_likelihood(Y,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the Generalized Pareto Distribution (μ=0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : numpy.array\n",
    "\t\t    observations\n",
    "        gamma : float\n",
    "            GPD index parameter\n",
    "        sigma : float\n",
    "            GPD scale parameter (>0)   \n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            log-likelihood of the sample Y to be drawn from a GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        n = Y.size\n",
    "        if gamma != 0:\n",
    "            tau = gamma/sigma\n",
    "            L = -n * log(sigma) - ( 1 + (1/gamma) ) * ( np.log(1+tau*Y) ).sum()\n",
    "        else:\n",
    "            L = n * ( 1 + log(Y.mean()) )\n",
    "        return L\n",
    "\n",
    "\n",
    "    def _grimshaw(self,epsilon = 1e-8, n_points = 10):\n",
    "        \"\"\"\n",
    "        Compute the GPD parameters estimation with the Grimshaw's trick\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : float\n",
    "\t\t    numerical parameter to perform (default : 1e-8)\n",
    "        n_points : int\n",
    "            maximum number of candidates for maximum likelihood (default : 10)\n",
    "        Returns\n",
    "        ----------\n",
    "        gamma_best,sigma_best,ll_best\n",
    "            gamma estimates, sigma estimates and corresponding log-likelihood\n",
    "        \"\"\"\n",
    "        def u(s):\n",
    "            return 1 + np.log(s).mean()\n",
    "            \n",
    "        def v(s):\n",
    "            return np.mean(1/s)\n",
    "        \n",
    "        def w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            return us*vs-1\n",
    "        \n",
    "        def jac_w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            jac_us = (1/t)*(1-vs)\n",
    "            jac_vs = (1/t)*(-vs+np.mean(1/s**2))\n",
    "            return us*jac_vs+vs*jac_us\n",
    "            \n",
    "    \n",
    "        Ym = self.peaks.min()\n",
    "        YM = self.peaks.max()\n",
    "        Ymean = self.peaks.mean()\n",
    "        \n",
    "        \n",
    "        a = -1/YM\n",
    "        if abs(a)<2*epsilon:\n",
    "            epsilon = abs(a)/n_points\n",
    "        \n",
    "        a = a + epsilon\n",
    "        b = 2*(Ymean-Ym)/(Ymean*Ym)\n",
    "        c = 2*(Ymean-Ym)/(Ym**2)\n",
    "    \n",
    "        # We look for possible roots\n",
    "        left_zeros = SPOT._rootsFinder(lambda t: w(self.peaks,t),\n",
    "                                 lambda t: jac_w(self.peaks,t),\n",
    "                                 (a+epsilon,-epsilon),\n",
    "                                 n_points,'regular')\n",
    "        \n",
    "        right_zeros = SPOT._rootsFinder(lambda t: w(self.peaks,t),\n",
    "                                  lambda t: jac_w(self.peaks,t),\n",
    "                                  (b,c),\n",
    "                                  n_points,'regular')\n",
    "    \n",
    "        # all the possible roots\n",
    "        zeros = np.concatenate((left_zeros,right_zeros))\n",
    "        \n",
    "        # 0 is always a solution so we initialize with it\n",
    "        gamma_best = 0\n",
    "        sigma_best = Ymean\n",
    "        ll_best = SPOT._log_likelihood(self.peaks,gamma_best,sigma_best)\n",
    "        \n",
    "        # we look for better candidates\n",
    "        for z in zeros:\n",
    "            gamma = u(1+z*self.peaks)-1\n",
    "            sigma = gamma/z\n",
    "            ll = SPOT._log_likelihood(self.peaks,gamma,sigma)\n",
    "            if ll>ll_best:\n",
    "                gamma_best = gamma\n",
    "                sigma_best = sigma\n",
    "                ll_best = ll\n",
    "    \n",
    "        return gamma_best,sigma_best,ll_best\n",
    "\n",
    "    \n",
    "\n",
    "    def _quantile(self,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the quantile at level 1-q\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float\n",
    "\t\t    GPD parameter\n",
    "        sigma : float\n",
    "            GPD parameter\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            quantile at level 1-q for the GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        r = self.n * self.proba / self.Nt\n",
    "        if gamma != 0:\n",
    "            return self.init_threshold + (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "        else:\n",
    "            return self.init_threshold - sigma*log(r)\n",
    "\n",
    "        \n",
    "    def run(self, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Run SPOT on the stream\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If False, SPOT will adapt the threshold assuming \\\n",
    "            there is no abnormal values\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            keys : 'thresholds' and 'alarms'\n",
    "            \n",
    "            'thresholds' contains the extreme quantiles and 'alarms' contains \\\n",
    "            the indexes of the values which have triggered alarms\n",
    "            \n",
    "        \"\"\"\n",
    "        if (self.n>self.init_data.size):\n",
    "            print('Warning : the algorithm seems to have already been run, you \\\n",
    "            should initialize before running again')\n",
    "            return {}\n",
    "        \n",
    "        # list of the thresholds\n",
    "        th = []\n",
    "        alarm = []\n",
    "        # Loop over the stream\n",
    "        for i in tqdm.tqdm(range(self.data.size)):\n",
    "    \n",
    "            # If the observed value exceeds the current threshold (alarm case)\n",
    "            if self.data[i]>self.extreme_quantile:\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks = np.append(self.peaks,self.data[i]-self.init_threshold)\n",
    "                    self.Nt += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw()\n",
    "                    self.extreme_quantile = self._quantile(g,s)\n",
    "\n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif self.data[i]>self.init_threshold:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks = np.append(self.peaks,self.data[i]-self.init_threshold)\n",
    "                    self.Nt += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw()\n",
    "                    self.extreme_quantile = self._quantile(g,s)\n",
    "            else:\n",
    "                self.n += 1\n",
    "\n",
    "                \n",
    "            th.append(self.extreme_quantile) # thresholds record\n",
    "        \n",
    "        return {'thresholds' : th, 'alarms': alarm}\n",
    "    \n",
    "\n",
    "    def plot(self,run_results,with_alarm = True):\n",
    "        \"\"\"\n",
    "        Plot the results of given by the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        run_results : dict\n",
    "            results given by the 'run' method\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If True, alarms are plotted.\n",
    "        Returns\n",
    "        ----------\n",
    "        list\n",
    "            list of the plots\n",
    "            \n",
    "        \"\"\"\n",
    "        x = range(self.data.size)\n",
    "        K = run_results.keys()\n",
    "        \n",
    "        ts_fig, = plt.plot(x,self.data,color=air_force_blue)\n",
    "        fig = [ts_fig]\n",
    "        \n",
    "        if 'thresholds' in K:\n",
    "            th = run_results['thresholds']\n",
    "            th_fig, = plt.plot(x,th,color=deep_saffron,lw=2,ls='dashed')\n",
    "            fig.append(th_fig)\n",
    "        \n",
    "        if with_alarm and ('alarms' in K):\n",
    "            alarm = run_results['alarms']\n",
    "            al_fig = plt.scatter(alarm,self.data[alarm],color='red')\n",
    "            fig.append(al_fig)\n",
    "            \n",
    "        plt.xlim((0,self.data.size))\n",
    "\n",
    "        \n",
    "        return fig\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "============================ UPPER & LOWER BOUNDS =============================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class biSPOT:\n",
    "    \"\"\"\n",
    "    This class allows to run biSPOT algorithm on univariate dataset (upper and lower bounds)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    proba : float\n",
    "        Detection level (risk), chosen by the user\n",
    "        \n",
    "    extreme_quantile : float\n",
    "        current threshold (bound between normal and abnormal events)\n",
    "        \n",
    "    data : numpy.array\n",
    "        stream\n",
    "    \n",
    "    init_data : numpy.array\n",
    "        initial batch of observations (for the calibration/initialization step)\n",
    "    \n",
    "    init_threshold : float\n",
    "        initial threshold computed during the calibration step\n",
    "    \n",
    "    peaks : numpy.array\n",
    "        array of peaks (excesses above the initial threshold)\n",
    "    \n",
    "    n : int\n",
    "        number of observed values\n",
    "    \n",
    "    Nt : int\n",
    "        number of observed peaks\n",
    "    \"\"\"\n",
    "    def __init__(self, q = 1e-4):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\t    Parameters\n",
    "\t    ----------\n",
    "\t    q\n",
    "\t\t    Detection level (risk)\n",
    "\t\n",
    "\t    Returns\n",
    "\t    ----------\n",
    "        biSPOT object\n",
    "        \"\"\"\n",
    "        self.proba = q\n",
    "        self.data = None\n",
    "        self.init_data = None\n",
    "        self.n = 0\n",
    "        nonedict =  {'up':None,'down':None}\n",
    "        \n",
    "        self.extreme_quantile = dict.copy(nonedict)\n",
    "        self.init_threshold = dict.copy(nonedict)\n",
    "        self.peaks = dict.copy(nonedict)\n",
    "        self.gamma = dict.copy(nonedict)\n",
    "        self.sigma = dict.copy(nonedict)\n",
    "        self.Nt = {'up':0,'down':0}\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        s += 'Streaming Peaks-Over-Threshold Object\\n'\n",
    "        s += 'Detection level q = %s\\n' % self.proba\n",
    "        if self.data is not None:\n",
    "            s += 'Data imported : Yes\\n'\n",
    "            s += '\\t initialization  : %s values\\n' % self.init_data.size\n",
    "            s += '\\t stream : %s values\\n' % self.data.size\n",
    "        else:\n",
    "            s += 'Data imported : No\\n'\n",
    "            return s\n",
    "            \n",
    "        if self.n == 0:\n",
    "            s += 'Algorithm initialized : No\\n'\n",
    "        else:\n",
    "            s += 'Algorithm initialized : Yes\\n'\n",
    "            s += '\\t initial threshold : %s\\n' % self.init_threshold\n",
    "            \n",
    "            r = self.n-self.init_data.size\n",
    "            if r > 0:\n",
    "                s += 'Algorithm run : Yes\\n'\n",
    "                s += '\\t number of observations : %s (%.2f %%)\\n' % (r,100*r/self.n)\n",
    "                s += '\\t triggered alarms : %s (%.2f %%)\\n' % (len(self.alarm),100*len(self.alarm)/self.n)\n",
    "            else:\n",
    "                s += '\\t number of peaks  : %s\\n' % self.Nt\n",
    "                s += '\\t upper extreme quantile : %s\\n' % self.extreme_quantile['up']\n",
    "                s += '\\t lower extreme quantile : %s\\n' % self.extreme_quantile['down']\n",
    "                s += 'Algorithm run : No\\n'\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def fit(self,init_data,data):\n",
    "        \"\"\"\n",
    "        Import data to biSPOT object\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    init_data : list, numpy.array or pandas.Series\n",
    "\t\t    initial batch to calibrate the algorithm ()\n",
    "            \n",
    "        data : numpy.array\n",
    "\t\t    data for the run (list, np.array or pd.series)\n",
    "\t\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            self.data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            self.data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            self.data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "            \n",
    "        if isinstance(init_data,list):\n",
    "            self.init_data = np.array(init_data)\n",
    "        elif isinstance(init_data,np.ndarray):\n",
    "            self.init_data = init_data\n",
    "        elif isinstance(init_data,pd.Series):\n",
    "            self.init_data = init_data.values\n",
    "        elif isinstance(init_data,int):\n",
    "            self.init_data = self.data[:init_data]\n",
    "            self.data = self.data[init_data:]\n",
    "        elif isinstance(init_data,float) & (init_data<1) & (init_data>0):\n",
    "            r = int(init_data*data.size)\n",
    "            self.init_data = self.data[:r]\n",
    "            self.data = self.data[r:]\n",
    "        else:\n",
    "            print('The initial data cannot be set')\n",
    "            return\n",
    "        \n",
    "    def add(self,data):\n",
    "        \"\"\"\n",
    "        This function allows to append data to the already fitted data\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    data : list, numpy.array, pandas.Series\n",
    "\t\t    data to append\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "        \n",
    "        self.data = np.append(self.data,data)\n",
    "        return\n",
    "\n",
    "    def initialize(self, verbose = True):\n",
    "        \"\"\"\n",
    "        Run the calibration (initialization) step\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    verbose : bool\n",
    "\t\t    (default = True) If True, gives details about the batch initialization\n",
    "        \"\"\"\n",
    "        n_init = self.init_data.size\n",
    "        \n",
    "        S = np.sort(self.init_data)     # we sort X to get the empirical quantile\n",
    "        self.init_threshold['up'] = S[int(0.98*n_init)] # t is fixed for the whole algorithm\n",
    "        self.init_threshold['down'] = S[int(0.02*n_init)] # t is fixed for the whole algorithm\n",
    "\n",
    "        # initial peaks\n",
    "        self.peaks['up'] = self.init_data[self.init_data>self.init_threshold['up']]-self.init_threshold['up']\n",
    "        self.peaks['down'] = -(self.init_data[self.init_data<self.init_threshold['down']]-self.init_threshold['down'])\n",
    "        self.Nt['up'] = self.peaks['up'].size\n",
    "        self.Nt['down'] = self.peaks['down'].size\n",
    "        self.n = n_init\n",
    "        \n",
    "        if verbose:\n",
    "            print('Initial threshold : %s' % self.init_threshold)\n",
    "            print('Number of peaks : %s' % self.Nt)\n",
    "            print('Grimshaw maximum log-likelihood estimation ... ', end = '')\n",
    "            \n",
    "        l = {'up':None,'down':None}\n",
    "        for side in ['up','down']:\n",
    "            g,s,l[side] = self._grimshaw(side)\n",
    "            self.extreme_quantile[side] = self._quantile(side,g,s)\n",
    "            self.gamma[side] = g\n",
    "            self.sigma[side] = s\n",
    "        \n",
    "        ltab = 20\n",
    "        form = ('\\t'+'%20s' + '%20.2f' + '%20.2f')\n",
    "        if verbose:\n",
    "            print('[done]')\n",
    "            print('\\t' + 'Parameters'.rjust(ltab) + 'Upper'.rjust(ltab) + 'Lower'.rjust(ltab))\n",
    "            print('\\t' + '-'*ltab*3)\n",
    "            print(form % (chr(0x03B3),self.gamma['up'],self.gamma['down']))\n",
    "            print(form % (chr(0x03C3),self.sigma['up'],self.sigma['down']))\n",
    "            print(form % ('likelihood',l['up'],l['down']))\n",
    "            print(form % ('Extreme quantile',self.extreme_quantile['up'],self.extreme_quantile['down']))\n",
    "            print('\\t' + '-'*ltab*3)\n",
    "        return \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _rootsFinder(fun,jac,bounds,npoints,method):\n",
    "        \"\"\"\n",
    "        Find possible roots of a scalar function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fun : function\n",
    "\t\t    scalar function \n",
    "        jac : function\n",
    "            first order derivative of the function  \n",
    "        bounds : tuple\n",
    "            (min,max) interval for the roots search    \n",
    "        npoints : int\n",
    "            maximum number of roots to output      \n",
    "        method : str\n",
    "            'regular' : regular sample of the search interval, 'random' : uniform (distribution) sample of the search interval\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        numpy.array\n",
    "            possible roots of the function\n",
    "        \"\"\"\n",
    "        if method == 'regular':\n",
    "            step = (bounds[1]-bounds[0])/(npoints+1)\n",
    "            X0 = np.arange(bounds[0]+step,bounds[1],step)\n",
    "        elif method == 'random':\n",
    "            X0 = np.random.uniform(bounds[0],bounds[1],npoints)\n",
    "        \n",
    "        def objFun(X,f,jac):\n",
    "            g = 0\n",
    "            j = np.zeros(X.shape)\n",
    "            i = 0\n",
    "            for x in X:\n",
    "                fx = f(x)\n",
    "                g = g+fx**2\n",
    "                j[i] = 2*fx*jac(x)\n",
    "                i = i+1\n",
    "            return g,j\n",
    "        opt = minimize(lambda X:objFun(X,fun,jac), X0, \n",
    "                       method='L-BFGS-B', \n",
    "                       jac=True, bounds=[bounds]*len(X0))\n",
    "        \n",
    "        X = opt.x\n",
    "        np.round(X,decimals = 5)\n",
    "        return np.unique(X)\n",
    "    \n",
    "    \n",
    "    def _log_likelihood(Y,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the Generalized Pareto Distribution (μ=0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : numpy.array\n",
    "\t\t    observations\n",
    "        gamma : float\n",
    "            GPD index parameter\n",
    "        sigma : float\n",
    "            GPD scale parameter (>0)   \n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            log-likelihood of the sample Y to be drawn from a GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        n = Y.size\n",
    "        if gamma != 0:\n",
    "            tau = gamma/sigma\n",
    "            L = -n * log(sigma) - ( 1 + (1/gamma) ) * ( np.log(1+tau*Y) ).sum()\n",
    "        else:\n",
    "            L = n * ( 1 + log(Y.mean()) )\n",
    "        return L\n",
    "\n",
    "\n",
    "    def _grimshaw(self,side,epsilon = 1e-8, n_points = 10):\n",
    "        \"\"\"\n",
    "        Compute the GPD parameters estimation with the Grimshaw's trick\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : float\n",
    "\t\t    numerical parameter to perform (default : 1e-8)\n",
    "        n_points : int\n",
    "            maximum number of candidates for maximum likelihood (default : 10)\n",
    "        Returns\n",
    "        ----------\n",
    "        gamma_best,sigma_best,ll_best\n",
    "            gamma estimates, sigma estimates and corresponding log-likelihood\n",
    "        \"\"\"\n",
    "        def u(s):\n",
    "            return 1 + np.log(s).mean()\n",
    "            \n",
    "        def v(s):\n",
    "            return np.mean(1/s)\n",
    "        \n",
    "        def w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            return us*vs-1\n",
    "        \n",
    "        def jac_w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            jac_us = (1/t)*(1-vs)\n",
    "            jac_vs = (1/t)*(-vs+np.mean(1/s**2))\n",
    "            return us*jac_vs+vs*jac_us\n",
    "            \n",
    "    \n",
    "        Ym = self.peaks[side].min()\n",
    "        YM = self.peaks[side].max()\n",
    "        Ymean = self.peaks[side].mean()\n",
    "        \n",
    "        \n",
    "        a = -1/YM\n",
    "        if abs(a)<2*epsilon:\n",
    "            epsilon = abs(a)/n_points\n",
    "        \n",
    "        a = a + epsilon\n",
    "        b = 2*(Ymean-Ym)/(Ymean*Ym)\n",
    "        c = 2*(Ymean-Ym)/(Ym**2)\n",
    "    \n",
    "        # We look for possible roots\n",
    "        left_zeros = biSPOT._rootsFinder(lambda t: w(self.peaks[side],t),\n",
    "                                 lambda t: jac_w(self.peaks[side],t),\n",
    "                                 (a+epsilon,-epsilon),\n",
    "                                 n_points,'regular')\n",
    "        \n",
    "        right_zeros = biSPOT._rootsFinder(lambda t: w(self.peaks[side],t),\n",
    "                                  lambda t: jac_w(self.peaks[side],t),\n",
    "                                  (b,c),\n",
    "                                  n_points,'regular')\n",
    "    \n",
    "        # all the possible roots\n",
    "        zeros = np.concatenate((left_zeros,right_zeros))\n",
    "        \n",
    "        # 0 is always a solution so we initialize with it\n",
    "        gamma_best = 0\n",
    "        sigma_best = Ymean\n",
    "        ll_best = biSPOT._log_likelihood(self.peaks[side],gamma_best,sigma_best)\n",
    "        \n",
    "        # we look for better candidates\n",
    "        for z in zeros:\n",
    "            gamma = u(1+z*self.peaks[side])-1\n",
    "            sigma = gamma/z\n",
    "            ll = biSPOT._log_likelihood(self.peaks[side],gamma,sigma)\n",
    "            if ll>ll_best:\n",
    "                gamma_best = gamma\n",
    "                sigma_best = sigma\n",
    "                ll_best = ll\n",
    "    \n",
    "        return gamma_best,sigma_best,ll_best\n",
    "\n",
    "    \n",
    "\n",
    "    def _quantile(self,side,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the quantile at level 1-q for a given side\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        side : str\n",
    "            'up' or 'down'\n",
    "        gamma : float\n",
    "\t\t    GPD parameter\n",
    "        sigma : float\n",
    "            GPD parameter\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            quantile at level 1-q for the GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        if side == 'up':\n",
    "            r = self.n * self.proba / self.Nt[side]\n",
    "            if gamma != 0:\n",
    "                return self.init_threshold['up'] + (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "            else:\n",
    "                return self.init_threshold['up'] - sigma*log(r)\n",
    "        elif side == 'down':\n",
    "            r = self.n * self.proba / self.Nt[side]\n",
    "            if gamma != 0:\n",
    "                return self.init_threshold['down'] - (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "            else:\n",
    "                return self.init_threshold['down'] + sigma*log(r)\n",
    "        else:\n",
    "            print('error : the side is not right')\n",
    "\n",
    "        \n",
    "    def run(self, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Run biSPOT on the stream\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If False, SPOT will adapt the threshold assuming \\\n",
    "            there is no abnormal values\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            keys : 'upper_thresholds', 'lower_thresholds' and 'alarms'\n",
    "            \n",
    "            '***-thresholds' contains the extreme quantiles and 'alarms' contains \\\n",
    "            the indexes of the values which have triggered alarms\n",
    "            \n",
    "        \"\"\"\n",
    "        if (self.n>self.init_data.size):\n",
    "            print('Warning : the algorithm seems to have already been run, you \\\n",
    "            should initialize before running again')\n",
    "            return {}\n",
    "        \n",
    "        # list of the thresholds\n",
    "        thup = []\n",
    "        thdown = []\n",
    "        alarm = []\n",
    "        # Loop over the stream\n",
    "        for i in tqdm.tqdm(range(self.data.size)):\n",
    "    \n",
    "            # If the observed value exceeds the current threshold (alarm case)\n",
    "            if self.data[i]>self.extreme_quantile['up'] :\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks['up'] = np.append(self.peaks['up'],self.data[i]-self.init_threshold['up'])\n",
    "                    self.Nt['up'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('up')\n",
    "                    self.extreme_quantile['up'] = self._quantile('up',g,s)\n",
    "\n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif self.data[i]>self.init_threshold['up']:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks['up'] = np.append(self.peaks['up'],self.data[i]-self.init_threshold['up'])\n",
    "                    self.Nt['up'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('up')\n",
    "                    self.extreme_quantile['up'] = self._quantile('up',g,s)\n",
    "                    \n",
    "            elif self.data[i]<self.extreme_quantile['down'] :\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks['down'] = np.append(self.peaks['down'],-(self.data[i]-self.init_threshold['down']))\n",
    "                    self.Nt['down'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('down')\n",
    "                    self.extreme_quantile['down'] = self._quantile('down',g,s)\n",
    "\n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif self.data[i]<self.init_threshold['down']:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks['down'] = np.append(self.peaks['down'],-(self.data[i]-self.init_threshold['down']))\n",
    "                    self.Nt['down'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('down')\n",
    "                    self.extreme_quantile['down'] = self._quantile('down',g,s)\n",
    "            else:\n",
    "                self.n += 1\n",
    "\n",
    "                \n",
    "            thup.append(self.extreme_quantile['up']) # thresholds record\n",
    "            thdown.append(self.extreme_quantile['down']) # thresholds record\n",
    "        \n",
    "        return {'upper_thresholds' : thup,'lower_thresholds' : thdown, 'alarms': alarm}\n",
    "    \n",
    "    def plot(self,run_results,with_alarm = True):\n",
    "        \"\"\"\n",
    "        Plot the results of given by the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        run_results : dict\n",
    "            results given by the 'run' method\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If True, alarms are plotted.\n",
    "        Returns\n",
    "        ----------\n",
    "        list\n",
    "            list of the plots\n",
    "            \n",
    "        \"\"\"\n",
    "        x = range(self.data.size)\n",
    "        K = run_results.keys()\n",
    "        \n",
    "        ts_fig, = plt.plot(x,self.data,color=air_force_blue)\n",
    "        fig = [ts_fig]\n",
    "        \n",
    "        if 'upper_thresholds' in K:\n",
    "            thup = run_results['upper_thresholds']\n",
    "            uth_fig, = plt.plot(x,thup,color=deep_saffron,lw=2,ls='dashed')\n",
    "            fig.append(uth_fig)\n",
    "            \n",
    "        if 'lower_thresholds' in K:\n",
    "            thdown = run_results['lower_thresholds']\n",
    "            lth_fig, = plt.plot(x,thdown,color=deep_saffron,lw=2,ls='dashed')\n",
    "            fig.append(lth_fig)\n",
    "        \n",
    "        if with_alarm and ('alarms' in K):\n",
    "            alarm = run_results['alarms']\n",
    "            al_fig = plt.scatter(alarm,self.data[alarm],color='red')\n",
    "            fig.append(al_fig)\n",
    "            \n",
    "        plt.xlim((0,self.data.size))\n",
    "\n",
    "        \n",
    "        return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "================================= WITH DRIFT ==================================\n",
    "\"\"\"\n",
    "\n",
    "def backMean(X,d):\n",
    "    M = []\n",
    "    w = X[:d].sum()\n",
    "    M.append(w/d)\n",
    "    for i in range(d,len(X)):\n",
    "        w = w - X[i-d] + X[i]\n",
    "        M.append(w/d)\n",
    "    return np.array(M)\n",
    "\n",
    "\n",
    "\n",
    "class dSPOT:\n",
    "    \"\"\"\n",
    "    This class allows to run DSPOT algorithm on univariate dataset (upper-bound)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    proba : float\n",
    "        Detection level (risk), chosen by the user\n",
    "        \n",
    "    depth : int\n",
    "        Number of observations to compute the moving average\n",
    "        \n",
    "    extreme_quantile : float\n",
    "        current threshold (bound between normal and abnormal events)\n",
    "        \n",
    "    data : numpy.array\n",
    "        stream\n",
    "    \n",
    "    init_data : numpy.array\n",
    "        initial batch of observations (for the calibration/initialization step)\n",
    "    \n",
    "    init_threshold : float\n",
    "        initial threshold computed during the calibration step\n",
    "    \n",
    "    peaks : numpy.array\n",
    "        array of peaks (excesses above the initial threshold)\n",
    "    \n",
    "    n : int\n",
    "        number of observed values\n",
    "    \n",
    "    Nt : int\n",
    "        number of observed peaks\n",
    "    \"\"\"\n",
    "    def __init__(self, q, depth):\n",
    "        self.proba = q\n",
    "        self.extreme_quantile = None\n",
    "        self.data = None\n",
    "        self.init_data = None\n",
    "        self.init_threshold = None\n",
    "        self.peaks = None\n",
    "        self.n = 0\n",
    "        self.Nt = 0\n",
    "        self.depth = depth\n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        s += 'Streaming Peaks-Over-Threshold Object\\n'\n",
    "        s += 'Detection level q = %s\\n' % self.proba\n",
    "        if self.data is not None:\n",
    "            s += 'Data imported : Yes\\n'\n",
    "            s += '\\t initialization  : %s values\\n' % self.init_data.size\n",
    "            s += '\\t stream : %s values\\n' % self.data.size\n",
    "        else:\n",
    "            s += 'Data imported : No\\n'\n",
    "            return s\n",
    "            \n",
    "        if self.n == 0:\n",
    "            s += 'Algorithm initialized : No\\n'\n",
    "        else:\n",
    "            s += 'Algorithm initialized : Yes\\n'\n",
    "            s += '\\t initial threshold : %s\\n' % self.init_threshold\n",
    "            \n",
    "            r = self.n-self.init_data.size\n",
    "            if r > 0:\n",
    "                s += 'Algorithm run : Yes\\n'\n",
    "                s += '\\t number of observations : %s (%.2f %%)\\n' % (r,100*r/self.n)\n",
    "                s += '\\t triggered alarms : %s (%.2f %%)\\n' % (len(self.alarm),100*len(self.alarm)/self.n)\n",
    "            else:\n",
    "                s += '\\t number of peaks  : %s\\n' % self.Nt\n",
    "                s += '\\t extreme quantile : %s\\n' % self.extreme_quantile\n",
    "                s += 'Algorithm run : No\\n'\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def fit(self,init_data,data):\n",
    "        \"\"\"\n",
    "        Import data to DSPOT object\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    init_data : list, numpy.array or pandas.Series\n",
    "\t\t    initial batch to calibrate the algorithm\n",
    "            \n",
    "        data : numpy.array\n",
    "\t\t    data for the run (list, np.array or pd.series)\n",
    "\t\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            self.data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            self.data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            self.data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "            \n",
    "        if isinstance(init_data,list):\n",
    "            self.init_data = np.array(init_data)\n",
    "        elif isinstance(init_data,np.ndarray):\n",
    "            self.init_data = init_data\n",
    "        elif isinstance(init_data,pd.Series):\n",
    "            self.init_data = init_data.values\n",
    "        elif isinstance(init_data,int):\n",
    "            self.init_data = self.data[:init_data]\n",
    "            self.data = self.data[init_data:]\n",
    "        elif isinstance(init_data,float) & (init_data<1) & (init_data>0):\n",
    "            r = int(init_data*data.size)\n",
    "            self.init_data = self.data[:r]\n",
    "            self.data = self.data[r:]\n",
    "        else:\n",
    "            print('The initial data cannot be set')\n",
    "            return\n",
    "        \n",
    "    def add(self,data):\n",
    "        \"\"\"\n",
    "        This function allows to append data to the already fitted data\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    data : list, numpy.array, pandas.Series\n",
    "\t\t    data to append\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "        \n",
    "        self.data = np.append(self.data,data)\n",
    "        return\n",
    "    \n",
    "    def initialize(self, verbose = True):\n",
    "        \"\"\"\n",
    "        Run the calibration (initialization) step\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    verbose : bool\n",
    "\t\t    (default = True) If True, gives details about the batch initialization\n",
    "        \"\"\"\n",
    "        n_init = self.init_data.size - self.depth\n",
    "        \n",
    "        M = backMean(self.init_data,self.depth)\n",
    "        T = self.init_data[self.depth:]-M[:-1] # new variable\n",
    "        \n",
    "        S = np.sort(T)     # we sort X to get the empirical quantile\n",
    "        self.init_threshold = S[int(0.98*n_init)] # t is fixed for the whole algorithm\n",
    "\n",
    "        # initial peaks\n",
    "        self.peaks = T[T>self.init_threshold]-self.init_threshold \n",
    "        self.Nt = self.peaks.size\n",
    "        self.n = n_init\n",
    "        \n",
    "        if verbose:\n",
    "            print('Initial threshold : %s' % self.init_threshold)\n",
    "            print('Number of peaks : %s' % self.Nt)\n",
    "            print('Grimshaw maximum log-likelihood estimation ... ', end = '')\n",
    "            \n",
    "        g,s,l = self._grimshaw()\n",
    "        self.extreme_quantile = self._quantile(g,s)\n",
    "        \n",
    "        if verbose:\n",
    "            print('[done]')\n",
    "            print('\\t'+chr(0x03B3) + ' = ' + str(g))\n",
    "            print('\\t'+chr(0x03C3) + ' = ' + str(s))\n",
    "            print('\\tL = ' + str(l))\n",
    "            print('Extreme quantile (probability = %s): %s' % (self.proba,self.extreme_quantile))\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _rootsFinder(fun,jac,bounds,npoints,method):\n",
    "        \"\"\"\n",
    "        Find possible roots of a scalar function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fun : function\n",
    "\t\t    scalar function \n",
    "        jac : function\n",
    "            first order derivative of the function  \n",
    "        bounds : tuple\n",
    "            (min,max) interval for the roots search    \n",
    "        npoints : int\n",
    "            maximum number of roots to output      \n",
    "        method : str\n",
    "            'regular' : regular sample of the search interval, 'random' : uniform (distribution) sample of the search interval\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        numpy.array\n",
    "            possible roots of the function\n",
    "        \"\"\"\n",
    "        if method == 'regular':\n",
    "            step = (bounds[1]-bounds[0])/(npoints+1)\n",
    "            X0 = np.arange(bounds[0]+step,bounds[1],step)\n",
    "        elif method == 'random':\n",
    "            X0 = np.random.uniform(bounds[0],bounds[1],npoints)\n",
    "        \n",
    "        def objFun(X,f,jac):\n",
    "            g = 0\n",
    "            j = np.zeros(X.shape)\n",
    "            i = 0\n",
    "            for x in X:\n",
    "                fx = f(x)\n",
    "                g = g+fx**2\n",
    "                j[i] = 2*fx*jac(x)\n",
    "                i = i+1\n",
    "            return g,j\n",
    "        \n",
    "        opt = minimize(lambda X:objFun(X,fun,jac), X0, \n",
    "                       method='L-BFGS-B', \n",
    "                       jac=True, bounds=[bounds]*len(X0))\n",
    "        \n",
    "        X = opt.x\n",
    "        np.round(X,decimals = 5)\n",
    "        return np.unique(X)\n",
    "    \n",
    "    \n",
    "    def _log_likelihood(Y,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the Generalized Pareto Distribution (μ=0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : numpy.array\n",
    "\t\t    observations\n",
    "        gamma : float\n",
    "            GPD index parameter\n",
    "        sigma : float\n",
    "            GPD scale parameter (>0)   \n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            log-likelihood of the sample Y to be drawn from a GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        n = Y.size\n",
    "        if gamma != 0:\n",
    "            tau = gamma/sigma\n",
    "            L = -n * log(sigma) - ( 1 + (1/gamma) ) * ( np.log(1+tau*Y) ).sum()\n",
    "        else:\n",
    "            L = n * ( 1 + log(Y.mean()) )\n",
    "        return L\n",
    "\n",
    "\n",
    "    def _grimshaw(self,epsilon = 1e-8, n_points = 10):\n",
    "        \"\"\"\n",
    "        Compute the GPD parameters estimation with the Grimshaw's trick\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : float\n",
    "\t\t    numerical parameter to perform (default : 1e-8)\n",
    "        n_points : int\n",
    "            maximum number of candidates for maximum likelihood (default : 10)\n",
    "        Returns\n",
    "        ----------\n",
    "        gamma_best,sigma_best,ll_best\n",
    "            gamma estimates, sigma estimates and corresponding log-likelihood\n",
    "        \"\"\"\n",
    "        def u(s):\n",
    "            return 1 + np.log(s).mean()\n",
    "            \n",
    "        def v(s):\n",
    "            return np.mean(1/s)\n",
    "        \n",
    "        def w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            return us*vs-1\n",
    "        \n",
    "        def jac_w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            jac_us = (1/t)*(1-vs)\n",
    "            jac_vs = (1/t)*(-vs+np.mean(1/s**2))\n",
    "            return us*jac_vs+vs*jac_us\n",
    "            \n",
    "    \n",
    "        Ym = self.peaks.min()\n",
    "        YM = self.peaks.max()\n",
    "        Ymean = self.peaks.mean()\n",
    "        \n",
    "        \n",
    "        a = -1/YM\n",
    "        if abs(a)<2*epsilon:\n",
    "            epsilon = abs(a)/n_points\n",
    "        \n",
    "        a = a + epsilon\n",
    "        b = 2*(Ymean-Ym)/(Ymean*Ym)\n",
    "        c = 2*(Ymean-Ym)/(Ym**2)\n",
    "    \n",
    "        # We look for possible roots\n",
    "        left_zeros = SPOT._rootsFinder(lambda t: w(self.peaks,t),\n",
    "                                 lambda t: jac_w(self.peaks,t),\n",
    "                                 (a+epsilon,-epsilon),\n",
    "                                 n_points,'regular')\n",
    "        \n",
    "        right_zeros = SPOT._rootsFinder(lambda t: w(self.peaks,t),\n",
    "                                  lambda t: jac_w(self.peaks,t),\n",
    "                                  (b,c),\n",
    "                                  n_points,'regular')\n",
    "    \n",
    "        # all the possible roots\n",
    "        zeros = np.concatenate((left_zeros,right_zeros))\n",
    "        \n",
    "        # 0 is always a solution so we initialize with it\n",
    "        gamma_best = 0\n",
    "        sigma_best = Ymean\n",
    "        ll_best = SPOT._log_likelihood(self.peaks,gamma_best,sigma_best)\n",
    "        \n",
    "        # we look for better candidates\n",
    "        for z in zeros:\n",
    "            gamma = u(1+z*self.peaks)-1\n",
    "            sigma = gamma/z\n",
    "            ll = dSPOT._log_likelihood(self.peaks,gamma,sigma)\n",
    "            if ll>ll_best:\n",
    "                gamma_best = gamma\n",
    "                sigma_best = sigma\n",
    "                ll_best = ll\n",
    "    \n",
    "        return gamma_best,sigma_best,ll_best\n",
    "\n",
    "    \n",
    "\n",
    "    def _quantile(self,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the quantile at level 1-q\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float\n",
    "\t\t    GPD parameter\n",
    "        sigma : float\n",
    "            GPD parameter\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            quantile at level 1-q for the GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        r = self.n * self.proba / self.Nt\n",
    "        if gamma != 0:\n",
    "            return self.init_threshold + (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "        else:\n",
    "            return self.init_threshold - sigma*log(r)\n",
    "\n",
    "        \n",
    "    def run(self, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Run biSPOT on the stream\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If False, SPOT will adapt the threshold assuming \\\n",
    "            there is no abnormal values\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            keys : 'upper_thresholds', 'lower_thresholds' and 'alarms'\n",
    "            \n",
    "            '***-thresholds' contains the extreme quantiles and 'alarms' contains \\\n",
    "            the indexes of the values which have triggered alarms\n",
    "            \n",
    "        \"\"\"\n",
    "        if (self.n>self.init_data.size):\n",
    "            print('Warning : the algorithm seems to have already been run, you \\\n",
    "            should initialize before running again')\n",
    "            return {}\n",
    "        \n",
    "        # actual normal window\n",
    "        W = self.init_data[-self.depth:]\n",
    "        \n",
    "        # list of the thresholds\n",
    "        th = []\n",
    "        alarm = []\n",
    "        # Loop over the stream\n",
    "        for i in tqdm.tqdm(range(self.data.size)):\n",
    "            Mi = W.mean()\n",
    "            # If the observed value exceeds the current threshold (alarm case)\n",
    "            if (self.data[i]-Mi)>self.extreme_quantile:\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks = np.append(self.peaks,self.data[i]-Mi-self.init_threshold)\n",
    "                    self.Nt += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw()\n",
    "                    self.extreme_quantile = self._quantile(g,s) #+ Mi\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "\n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif (self.data[i]-Mi)>self.init_threshold:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks = np.append(self.peaks,self.data[i]-Mi-self.init_threshold)\n",
    "                    self.Nt += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw()\n",
    "                    self.extreme_quantile = self._quantile(g,s) #+ Mi\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "            else:\n",
    "                self.n += 1\n",
    "                W = np.append(W[1:],self.data[i])\n",
    "\n",
    "                \n",
    "            th.append(self.extreme_quantile+Mi) # thresholds record\n",
    "        \n",
    "        return {'thresholds' : th, 'alarms': alarm}\n",
    "    \n",
    "\n",
    "    def plot(self,run_results, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Plot the results given by the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        run_results : dict\n",
    "            results given by the 'run' method\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If True, alarms are plotted.\n",
    "        Returns\n",
    "        ----------\n",
    "        list\n",
    "            list of the plots\n",
    "            \n",
    "        \"\"\"\n",
    "        x = range(self.data.size)\n",
    "        K = run_results.keys()\n",
    "        \n",
    "        ts_fig, = plt.plot(x,self.data,color=air_force_blue,alpha=0.5)\n",
    "        fig = [ts_fig]\n",
    "        \n",
    "#        if 'upper_thresholds' in K:\n",
    "#            thup = run_results['upper_thresholds']\n",
    "#            uth_fig, = plt.plot(x,thup,color=deep_saffron,lw=2,ls='dashed')\n",
    "#            fig.append(uth_fig)\n",
    "#            \n",
    "#        if 'lower_thresholds' in K:\n",
    "#            thdown = run_results['lower_thresholds']\n",
    "#            lth_fig, = plt.plot(x,thdown,color=deep_saffron,lw=2,ls='dashed')\n",
    "#            fig.append(lth_fig)\n",
    "        \n",
    "        if 'thresholds' in K:\n",
    "            th = run_results['thresholds']\n",
    "            th_fig, = plt.plot(x,th,color=deep_saffron,lw=2,ls='dashed',alpha=0.5)\n",
    "            fig.append(th_fig)\n",
    "        \n",
    "        if with_alarm and ('alarms' in K):\n",
    "            alarm = run_results['alarms']\n",
    "            if len(alarm)>0:\n",
    "                plt.scatter(alarm,self.data[alarm],color='red')\n",
    "            \n",
    "        plt.xlim((0,self.data.size))\n",
    "\n",
    "        \n",
    "        return fig\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "=========================== DRIFT & DOUBLE BOUNDS =============================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class bidSPOT:\n",
    "    \"\"\"\n",
    "    This class allows to run DSPOT algorithm on univariate dataset (upper and lower bounds)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    proba : float\n",
    "        Detection level (risk), chosen by the user\n",
    "        \n",
    "    depth : int\n",
    "        Number of observations to compute the moving average\n",
    "        \n",
    "    extreme_quantile : float\n",
    "        current threshold (bound between normal and abnormal events)\n",
    "        \n",
    "    data : numpy.array\n",
    "        stream\n",
    "    \n",
    "    init_data : numpy.array\n",
    "        initial batch of observations (for the calibration/initialization step)\n",
    "    \n",
    "    init_threshold : float\n",
    "        initial threshold computed during the calibration step\n",
    "    \n",
    "    peaks : numpy.array\n",
    "        array of peaks (excesses above the initial threshold)\n",
    "    \n",
    "    n : int\n",
    "        number of observed values\n",
    "    \n",
    "    Nt : int\n",
    "        number of observed peaks\n",
    "    \"\"\"\n",
    "    def __init__(self, q = 1e-4, depth = 10):\n",
    "        self.proba = q\n",
    "        self.data = None\n",
    "        self.init_data = None\n",
    "        self.n = 0\n",
    "        self.depth = depth\n",
    "        \n",
    "        nonedict =  {'up':None,'down':None}\n",
    "        \n",
    "        self.extreme_quantile = dict.copy(nonedict)\n",
    "        self.init_threshold = dict.copy(nonedict)\n",
    "        self.peaks = dict.copy(nonedict)\n",
    "        self.gamma = dict.copy(nonedict)\n",
    "        self.sigma = dict.copy(nonedict)\n",
    "        self.Nt = {'up':0,'down':0}\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        s += 'Streaming Peaks-Over-Threshold Object\\n'\n",
    "        s += 'Detection level q = %s\\n' % self.proba\n",
    "        if self.data is not None:\n",
    "            s += 'Data imported : Yes\\n'\n",
    "            s += '\\t initialization  : %s values\\n' % self.init_data.size\n",
    "            s += '\\t stream : %s values\\n' % self.data.size\n",
    "        else:\n",
    "            s += 'Data imported : No\\n'\n",
    "            return s\n",
    "            \n",
    "        if self.n == 0:\n",
    "            s += 'Algorithm initialized : No\\n'\n",
    "        else:\n",
    "            s += 'Algorithm initialized : Yes\\n'\n",
    "            s += '\\t initial threshold : %s\\n' % self.init_threshold\n",
    "            \n",
    "            r = self.n-self.init_data.size\n",
    "            if r > 0:\n",
    "                s += 'Algorithm run : Yes\\n'\n",
    "                s += '\\t number of observations : %s (%.2f %%)\\n' % (r,100*r/self.n)\n",
    "                s += '\\t triggered alarms : %s (%.2f %%)\\n' % (len(self.alarm),100*len(self.alarm)/self.n)\n",
    "            else:\n",
    "                s += '\\t number of peaks  : %s\\n' % self.Nt\n",
    "                s += '\\t upper extreme quantile : %s\\n' % self.extreme_quantile['up']\n",
    "                s += '\\t lower extreme quantile : %s\\n' % self.extreme_quantile['down']\n",
    "                s += 'Algorithm run : No\\n'\n",
    "        return s\n",
    "    \n",
    "    \n",
    "    def fit(self,init_data,data):\n",
    "        \"\"\"\n",
    "        Import data to biDSPOT object\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    init_data : list, numpy.array or pandas.Series\n",
    "\t\t    initial batch to calibrate the algorithm\n",
    "            \n",
    "        data : numpy.array\n",
    "\t\t    data for the run (list, np.array or pd.series)\n",
    "\t\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            self.data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            self.data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            self.data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "            \n",
    "        if isinstance(init_data,list):\n",
    "            self.init_data = np.array(init_data)\n",
    "        elif isinstance(init_data,np.ndarray):\n",
    "            self.init_data = init_data\n",
    "        elif isinstance(init_data,pd.Series):\n",
    "            self.init_data = init_data.values\n",
    "        elif isinstance(init_data,int):\n",
    "            self.init_data = self.data[:init_data]\n",
    "            self.data = self.data[init_data:]\n",
    "        elif isinstance(init_data,float) & (init_data<1) & (init_data>0):\n",
    "            r = int(init_data*data.size)\n",
    "            self.init_data = self.data[:r]\n",
    "            self.data = self.data[r:]\n",
    "        else:\n",
    "            print('The initial data cannot be set')\n",
    "            return\n",
    "        \n",
    "    def add(self,data):\n",
    "        \"\"\"\n",
    "        This function allows to append data to the already fitted data\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    data : list, numpy.array, pandas.Series\n",
    "\t\t    data to append\n",
    "        \"\"\"\n",
    "        if isinstance(data,list):\n",
    "            data = np.array(data)\n",
    "        elif isinstance(data,np.ndarray):\n",
    "            data = data\n",
    "        elif isinstance(data,pd.Series):\n",
    "            data = data.values\n",
    "        else:\n",
    "            print('This data format (%s) is not supported' % type(data))\n",
    "            return\n",
    "        \n",
    "        self.data = np.append(self.data,data)\n",
    "        return\n",
    "    \n",
    "    def initialize(self, verbose = True):\n",
    "        \"\"\"\n",
    "        Run the calibration (initialization) step\n",
    "        \n",
    "        Parameters\n",
    "\t    ----------\n",
    "\t    verbose : bool\n",
    "\t\t    (default = True) If True, gives details about the batch initialization\n",
    "        \"\"\"\n",
    "        n_init = self.init_data.size - self.depth\n",
    "        \n",
    "        M = backMean(self.init_data,self.depth)\n",
    "        T = self.init_data[self.depth:]-M[:-1] # new variable\n",
    "        \n",
    "        S = np.sort(T)     # we sort T to get the empirical quantile\n",
    "        self.init_threshold['up'] = S[int(0.98*n_init)] # t is fixed for the whole algorithm\n",
    "        self.init_threshold['down'] = S[int(0.02*n_init)] # t is fixed for the whole algorithm\n",
    "\n",
    "        # initial peaks\n",
    "        self.peaks['up'] = T[T>self.init_threshold['up']]-self.init_threshold['up']\n",
    "        self.peaks['down'] = -( T[ T<self.init_threshold['down'] ] - self.init_threshold['down'] )\n",
    "        self.Nt['up'] = self.peaks['up'].size\n",
    "        self.Nt['down'] = self.peaks['down'].size\n",
    "        self.n = n_init\n",
    "        \n",
    "        if verbose:\n",
    "            print('Initial threshold : %s' % self.init_threshold)\n",
    "            print('Number of peaks : %s' % self.Nt)\n",
    "            print('Grimshaw maximum log-likelihood estimation ... ', end = '')\n",
    "            \n",
    "        l = {'up':None,'down':None}\n",
    "        for side in ['up','down']:\n",
    "            g,s,l[side] = self._grimshaw(side)\n",
    "            self.extreme_quantile[side] = self._quantile(side,g,s)\n",
    "            self.gamma[side] = g\n",
    "            self.sigma[side] = s\n",
    "        \n",
    "        ltab = 20\n",
    "        form = ('\\t'+'%20s' + '%20.2f' + '%20.2f')\n",
    "        if verbose:\n",
    "            print('[done]')\n",
    "            print('\\t' + 'Parameters'.rjust(ltab) + 'Upper'.rjust(ltab) + 'Lower'.rjust(ltab))\n",
    "            print('\\t' + '-'*ltab*3)\n",
    "            print(form % (chr(0x03B3),self.gamma['up'],self.gamma['down']))\n",
    "            print(form % (chr(0x03C3),self.sigma['up'],self.sigma['down']))\n",
    "            print(form % ('likelihood',l['up'],l['down']))\n",
    "            print(form % ('Extreme quantile',self.extreme_quantile['up'],self.extreme_quantile['down']))\n",
    "            print('\\t' + '-'*ltab*3)\n",
    "        return \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def _rootsFinder(fun,jac,bounds,npoints,method):\n",
    "        \"\"\"\n",
    "        Find possible roots of a scalar function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fun : function\n",
    "\t\t    scalar function \n",
    "        jac : function\n",
    "            first order derivative of the function  \n",
    "        bounds : tuple\n",
    "            (min,max) interval for the roots search    \n",
    "        npoints : int\n",
    "            maximum number of roots to output      \n",
    "        method : str\n",
    "            'regular' : regular sample of the search interval, 'random' : uniform (distribution) sample of the search interval\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        numpy.array\n",
    "            possible roots of the function\n",
    "        \"\"\"\n",
    "        if method == 'regular':\n",
    "            step = (bounds[1]-bounds[0])/(npoints+1)\n",
    "            X0 = np.arange(bounds[0]+step,bounds[1],step)\n",
    "        elif method == 'random':\n",
    "            X0 = np.random.uniform(bounds[0],bounds[1],npoints)\n",
    "        \n",
    "        def objFun(X,f,jac):\n",
    "            g = 0\n",
    "            j = np.zeros(X.shape)\n",
    "            i = 0\n",
    "            for x in X:\n",
    "                fx = f(x)\n",
    "                g = g+fx**2\n",
    "                j[i] = 2*fx*jac(x)\n",
    "                i = i+1\n",
    "            return g,j\n",
    "        \n",
    "        opt = minimize(lambda X:objFun(X,fun,jac), X0, \n",
    "                       method='L-BFGS-B', \n",
    "                       jac=True, bounds=[bounds]*len(X0))\n",
    "        \n",
    "        X = opt.x\n",
    "        np.round(X,decimals = 5)\n",
    "        return np.unique(X)\n",
    "    \n",
    "    \n",
    "    def _log_likelihood(Y,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood for the Generalized Pareto Distribution (μ=0)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Y : numpy.array\n",
    "\t\t    observations\n",
    "        gamma : float\n",
    "            GPD index parameter\n",
    "        sigma : float\n",
    "            GPD scale parameter (>0)   \n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            log-likelihood of the sample Y to be drawn from a GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        n = Y.size\n",
    "        if gamma != 0:\n",
    "            tau = gamma/sigma\n",
    "            L = -n * log(sigma) - ( 1 + (1/gamma) ) * ( np.log(1+tau*Y) ).sum()\n",
    "        else:\n",
    "            L = n * ( 1 + log(Y.mean()) )\n",
    "        return L\n",
    "\n",
    "\n",
    "    def _grimshaw(self,side,epsilon = 1e-8, n_points = 8):\n",
    "        \"\"\"\n",
    "        Compute the GPD parameters estimation with the Grimshaw's trick\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon : float\n",
    "\t\t    numerical parameter to perform (default : 1e-8)\n",
    "        n_points : int\n",
    "            maximum number of candidates for maximum likelihood (default : 10)\n",
    "        Returns\n",
    "        ----------\n",
    "        gamma_best,sigma_best,ll_best\n",
    "            gamma estimates, sigma estimates and corresponding log-likelihood\n",
    "        \"\"\"\n",
    "        def u(s):\n",
    "            return 1 + np.log(s).mean()\n",
    "            \n",
    "        def v(s):\n",
    "            return np.mean(1/s)\n",
    "        \n",
    "        def w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            return us*vs-1\n",
    "        \n",
    "        def jac_w(Y,t):\n",
    "            s = 1+t*Y\n",
    "            us = u(s)\n",
    "            vs = v(s)\n",
    "            jac_us = (1/t)*(1-vs)\n",
    "            jac_vs = (1/t)*(-vs+np.mean(1/s**2))\n",
    "            return us*jac_vs+vs*jac_us\n",
    "            \n",
    "    \n",
    "        Ym = self.peaks[side].min()\n",
    "        YM = self.peaks[side].max()\n",
    "        Ymean = self.peaks[side].mean()\n",
    "        \n",
    "        \n",
    "        a = -1/YM\n",
    "        if abs(a)<2*epsilon:\n",
    "            epsilon = abs(a)/n_points\n",
    "        \n",
    "        a = a + epsilon\n",
    "        b = 2*(Ymean-Ym)/(Ymean*Ym)\n",
    "        c = 2*(Ymean-Ym)/(Ym**2)\n",
    "    \n",
    "        # We look for possible roots\n",
    "        left_zeros = bidSPOT._rootsFinder(lambda t: w(self.peaks[side],t),\n",
    "                                 lambda t: jac_w(self.peaks[side],t),\n",
    "                                 (a+epsilon,-epsilon),\n",
    "                                 n_points,'regular')\n",
    "        \n",
    "        right_zeros = bidSPOT._rootsFinder(lambda t: w(self.peaks[side],t),\n",
    "                                  lambda t: jac_w(self.peaks[side],t),\n",
    "                                  (b,c),\n",
    "                                  n_points,'regular')\n",
    "    \n",
    "        # all the possible roots\n",
    "        zeros = np.concatenate((left_zeros,right_zeros))\n",
    "        \n",
    "        # 0 is always a solution so we initialize with it\n",
    "        gamma_best = 0\n",
    "        sigma_best = Ymean\n",
    "        ll_best = bidSPOT._log_likelihood(self.peaks[side],gamma_best,sigma_best)\n",
    "        \n",
    "        # we look for better candidates\n",
    "        for z in zeros:\n",
    "            gamma = u(1+z*self.peaks[side])-1\n",
    "            sigma = gamma/z\n",
    "            ll = bidSPOT._log_likelihood(self.peaks[side],gamma,sigma)\n",
    "            if ll>ll_best:\n",
    "                gamma_best = gamma\n",
    "                sigma_best = sigma\n",
    "                ll_best = ll\n",
    "    \n",
    "        return gamma_best,sigma_best,ll_best\n",
    "\n",
    "    \n",
    "\n",
    "    def _quantile(self,side,gamma,sigma):\n",
    "        \"\"\"\n",
    "        Compute the quantile at level 1-q for a given side\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        side : str\n",
    "            'up' or 'down'\n",
    "        gamma : float\n",
    "\t\t    GPD parameter\n",
    "        sigma : float\n",
    "            GPD parameter\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            quantile at level 1-q for the GPD(γ,σ,μ=0)\n",
    "        \"\"\"\n",
    "        if side == 'up':\n",
    "            r = self.n * self.proba / self.Nt[side]\n",
    "            if gamma != 0:\n",
    "                return self.init_threshold['up'] + (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "            else:\n",
    "                return self.init_threshold['up'] - sigma*log(r)\n",
    "        elif side == 'down':\n",
    "            r = self.n * self.proba / self.Nt[side]\n",
    "            if gamma != 0:\n",
    "                return self.init_threshold['down'] - (sigma/gamma)*(pow(r,-gamma)-1)\n",
    "            else:\n",
    "                return self.init_threshold['down'] + sigma*log(r)\n",
    "        else:\n",
    "            print('error : the side is not right')\n",
    "\n",
    "        \n",
    "    def run(self, with_alarm = True, plot = True):\n",
    "        \"\"\"\n",
    "        Run biDSPOT on the stream\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If False, SPOT will adapt the threshold assuming \\\n",
    "            there is no abnormal values\n",
    "        Returns\n",
    "        ----------\n",
    "        dict\n",
    "            keys : 'upper_thresholds', 'lower_thresholds' and 'alarms'\n",
    "            \n",
    "            '***-thresholds' contains the extreme quantiles and 'alarms' contains \\\n",
    "            the indexes of the values which have triggered alarms\n",
    "            \n",
    "        \"\"\"\n",
    "        if (self.n>self.init_data.size):\n",
    "            print('Warning : the algorithm seems to have already been run, you \\\n",
    "            should initialize before running again')\n",
    "            return {}\n",
    "        \n",
    "        # actual normal window\n",
    "        W = self.init_data[-self.depth:]\n",
    "        \n",
    "        # list of the thresholds\n",
    "        thup = []\n",
    "        thdown = []\n",
    "        alarm = []\n",
    "        # Loop over the stream\n",
    "        for i in tqdm.tqdm(range(self.data.size)):\n",
    "            Mi = W.mean()\n",
    "            Ni = self.data[i]-Mi\n",
    "            # If the observed value exceeds the current threshold (alarm case)\n",
    "            if Ni>self.extreme_quantile['up'] :\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks['up'] = np.append(self.peaks['up'],Ni-self.init_threshold['up'])\n",
    "                    self.Nt['up'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('up')\n",
    "                    self.extreme_quantile['up'] = self._quantile('up',g,s)\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "                    \n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif Ni>self.init_threshold['up']:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks['up'] = np.append(self.peaks['up'],Ni-self.init_threshold['up'])\n",
    "                    self.Nt['up'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "                    g,s,l = self._grimshaw('up')\n",
    "                    self.extreme_quantile['up'] = self._quantile('up',g,s)\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "                    \n",
    "            elif Ni<self.extreme_quantile['down'] :\n",
    "                # if we want to alarm, we put it in the alarm list\n",
    "                if with_alarm:\n",
    "                    alarm.append(i)\n",
    "                # otherwise we add it in the peaks\n",
    "                else:\n",
    "                    self.peaks['down'] = np.append(self.peaks['down'],-(Ni-self.init_threshold['down']))\n",
    "                    self.Nt['down'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('down')\n",
    "                    self.extreme_quantile['down'] = self._quantile('down',g,s)\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "                    \n",
    "            # case where the value exceeds the initial threshold but not the alarm ones\n",
    "            elif Ni<self.init_threshold['down']:\n",
    "                    # we add it in the peaks\n",
    "                    self.peaks['down'] = np.append(self.peaks['down'],-(Ni-self.init_threshold['down']))\n",
    "                    self.Nt['down'] += 1\n",
    "                    self.n += 1\n",
    "                    # and we update the thresholds\n",
    "\n",
    "                    g,s,l = self._grimshaw('down')\n",
    "                    self.extreme_quantile['down'] = self._quantile('down',g,s)\n",
    "                    W = np.append(W[1:],self.data[i])\n",
    "            else:\n",
    "                self.n += 1\n",
    "                W = np.append(W[1:],self.data[i])\n",
    "\n",
    "                \n",
    "            thup.append(self.extreme_quantile['up']+Mi) # upper thresholds record\n",
    "            thdown.append(self.extreme_quantile['down']+Mi) # lower thresholds record\n",
    "        \n",
    "        return {'upper_thresholds' : thup,'lower_thresholds' : thdown, 'alarms': alarm}\n",
    "    \n",
    "\n",
    "    def plot(self,run_results, with_alarm = True):\n",
    "        \"\"\"\n",
    "        Plot the results given by the run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        run_results : dict\n",
    "            results given by the 'run' method\n",
    "        with_alarm : bool\n",
    "\t\t    (default = True) If True, alarms are plotted.\n",
    "        Returns\n",
    "        ----------\n",
    "        list\n",
    "            list of the plots\n",
    "            \n",
    "        \"\"\"\n",
    "        x = range(self.data.size)\n",
    "        K = run_results.keys()\n",
    "        \n",
    "        ts_fig, = plt.plot(x,self.data,color=air_force_blue)\n",
    "        fig = [ts_fig]\n",
    "        \n",
    "        if 'upper_thresholds' in K:\n",
    "            thup = run_results['upper_thresholds']\n",
    "            uth_fig, = plt.plot(x,thup,color=deep_saffron,lw=2,ls='dashed',alpha=0.5)\n",
    "            fig.append(uth_fig)\n",
    "            \n",
    "        if 'lower_thresholds' in K:\n",
    "            thdown = run_results['lower_thresholds']\n",
    "            lth_fig, = plt.plot(x,thdown,color=deep_saffron,lw=2,ls='dashed',alpha=0.5)\n",
    "            fig.append(lth_fig)\n",
    "        \n",
    "        if with_alarm and ('alarms' in K):\n",
    "            alarm = run_results['alarms']\n",
    "            if len(alarm)>0:\n",
    "                al_fig = plt.scatter(alarm,self.data[alarm],color='red',alpha=0.5)\n",
    "                fig.append(al_fig)\n",
    "            \n",
    "        plt.xlim((0,self.data.size))\n",
    "\n",
    "        \n",
    "        return fig\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
    "    if args.lradj=='type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch-1) // 1))}\n",
    "    elif args.lradj=='type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6, \n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path+'/'+'checkpoint.pth')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp_gta_dad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_loader import (\n",
    "   STOCK_Hour\n",
    ")\n",
    "\n",
    "from utils.tools import EarlyStopping, adjust_learning_rate\n",
    "from utils.metrics import metric\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Exp_GTA_DAD(Exp_Basic):\n",
    "    def __init__(self, args):\n",
    "        super(Exp_GTA_DAD, self).__init__(args)\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model_dict = {\n",
    "            'gta':GTA,\n",
    "        }\n",
    "        print(self.args.model)\n",
    "        if self.args.model=='gta':\n",
    "            model = model_dict[self.args.model](\n",
    "                self.args.num_nodes,\n",
    "                self.args.seq_len, \n",
    "                self.args.pred_len, \n",
    "                self.args.num_levels,\n",
    "                self.args.factor,\n",
    "                self.args.d_model, \n",
    "                self.args.n_heads, \n",
    "                self.args.e_layers,\n",
    "                self.args.d_layers, \n",
    "                self.args.d_ff,\n",
    "                self.args.dropout, \n",
    "                self.args.attn,\n",
    "                self.args.embed,\n",
    "                self.args.data,\n",
    "                self.args.activation,\n",
    "                self.device\n",
    "            )\n",
    "        \n",
    "        return model.double()\n",
    "\n",
    "    def _get_data(self, flag):\n",
    "        args = self.args\n",
    "\n",
    "        data_dict = {\n",
    "            'General_Electronics_stock_data':STOCK_Hour,\n",
    "            \n",
    "        }\n",
    "        Data = data_dict[self.args.data]\n",
    "\n",
    "        if flag == 'test':\n",
    "            shuffle_flag = False; drop_last = True; batch_size = args.batch_size\n",
    "        else:\n",
    "            shuffle_flag = True; drop_last = True; batch_size = args.batch_size\n",
    "        print(args.features)\n",
    "        data_set = Data(\n",
    "            root_path=args.root_path,\n",
    "            data_path=args.data_path,\n",
    "            flag=flag,\n",
    "            size=[args.seq_len, args.pred_len],\n",
    "            features=args.features,\n",
    "            target=args.target\n",
    "        )\n",
    "        print(flag, len(data_set))\n",
    "        data_loader = DataLoader(\n",
    "            data_set,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle_flag,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=drop_last)\n",
    "\n",
    "        return data_set, data_loader\n",
    "\n",
    "    def _select_optimizer(self):\n",
    "        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        return model_optim\n",
    "    \n",
    "    def _select_criterion(self):\n",
    "        criterion =  nn.MSELoss()\n",
    "        return criterion\n",
    "\n",
    "    def vali(self, vali_data, vali_loader, criterion):\n",
    "        self.model.eval()\n",
    "        total_loss = []\n",
    "\n",
    "        for i, (batch_x,batch_y,batch_x_mark,batch_y_mark,batch_label) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.double().to(self.device)\n",
    "            batch_y = batch_y.double().to(self.device)\n",
    "\n",
    "            batch_x_mark = batch_x_mark.double().to(self.device)\n",
    "            batch_y_mark = batch_y_mark.double().to(self.device)\n",
    "\n",
    "            # decoder input\n",
    "            # dec_inp = torch.zeros_like(batch_y[:,-self.args.pred_len:,:]).double()\n",
    "            # dec_inp = torch.cat([batch_y[:,:self.args.label_len,:], dec_inp], dim=1).double().to(self.device)\n",
    "            # encoder - decoder\n",
    "            # outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            outputs = self.model(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "            batch_y = batch_y[:,-self.args.pred_len:,:].to(self.device)\n",
    "\n",
    "            pred = outputs.detach().cpu()\n",
    "            true = batch_y.detach().cpu()\n",
    "\n",
    "            loss = criterion(pred, true) \n",
    "\n",
    "            total_loss.append(loss)\n",
    "        \n",
    "        total_loss = np.average(total_loss)\n",
    "        self.model.train()\n",
    "        return total_loss\n",
    "        \n",
    "    def train(self, setting):\n",
    "        train_data, train_loader = self._get_data(flag = 'train')\n",
    "        print(\"Train_loader: \",train_loader)\n",
    "        vali_data, vali_loader = self._get_data(flag = 'val')\n",
    "        test_data, test_loader = self._get_data(flag = 'test')\n",
    "\n",
    "        path = './checkpoints/'+setting\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        time_now = time.time()\n",
    "        \n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n",
    "        \n",
    "        model_optim = self._select_optimizer()\n",
    "        criterion =  self._select_criterion()\n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "            \n",
    "            self.model.train()\n",
    "            \n",
    "            for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(train_loader): #try copying that file here and try \n",
    "                iter_count += 1\n",
    "                \n",
    "                model_optim.zero_grad()\n",
    "                \n",
    "                batch_x = batch_x.double().to(self.device)\n",
    "                batch_y = batch_y.double().to(self.device)\n",
    "                \n",
    "                batch_x_mark = batch_x_mark.double().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.double().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                # dec_inp = torch.zeros_like(batch_y[:,-self.args.pred_len:,:]).double()\n",
    "                # dec_inp = torch.cat([batch_y[:,:self.args.label_len,:], dec_inp], dim=1).double().to(self.device)\n",
    "                # encoder - decoder\n",
    "                # outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                outputs = self.model(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "                batch_y = batch_y[:,-self.args.pred_len:,:].to(self.device)\n",
    "\n",
    "                loss = criterion(outputs, batch_y) + \\\n",
    "                        torch.sum(torch.abs(self.model.gt_embedding.gc_module.logits[:, 0]))\n",
    "                train_loss.append(loss.item())\n",
    "                \n",
    "                if (i+1) % 100==0:\n",
    "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                    speed = (time.time()-time_now)/iter_count\n",
    "                    left_time = speed*((self.args.train_epochs - epoch)*train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "                \n",
    "                loss.backward()\n",
    "                model_optim.step()\n",
    "\n",
    "            train_loss = np.average(train_loss)\n",
    "            vali_loss = self.vali(vali_data, vali_loader, criterion)\n",
    "            test_loss = self.vali(test_data, test_loader, criterion)\n",
    "\n",
    "            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping(vali_loss, self.model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            adjust_learning_rate(model_optim, epoch+1, self.args)\n",
    "            \n",
    "        best_model_path = path+'/'+'checkpoint.pth'\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def test(self, setting):\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        preds = []\n",
    "        trues = []\n",
    "        labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x,batch_y,batch_x_mark,batch_y_mark,batch_label) in enumerate(test_loader):\n",
    "                batch_x = batch_x.double().to(self.device)\n",
    "                batch_y = batch_y.double().to(self.device)\n",
    "                batch_x_mark = batch_x_mark.double().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.double().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                # dec_inp = torch.zeros_like(batch_y[:,-self.args.pred_len:,:]).double()\n",
    "                # dec_inp = torch.cat([batch_y[:,:self.args.label_len,:], dec_inp], dim=1).double().to(self.device)\n",
    "                # encoder - decoder\n",
    "                # outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                outputs = self.model(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "                batch_y = batch_y[:,-self.args.pred_len:,:].to(self.device)\n",
    "\n",
    "                pred = outputs.detach().cpu().numpy()#.squeeze()\n",
    "                true = batch_y.detach().cpu().numpy()#.squeeze()\n",
    "                batch_label = batch_label.long().detach().numpy()\n",
    "                \n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "                labels.append(batch_label)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        trues = np.array(trues)\n",
    "        labels = np.array(labels)\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
    "        labels = labels.reshape(-1, labels.shape[-1])\n",
    "        print('test shape:', preds.shape, trues.shape)\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting +'/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        mae, mse, rmse, mape, mspe = metric(preds, trues)\n",
    "        print('mse:{}, mae:{}'.format(mse, mae))\n",
    "\n",
    "        np.save(folder_path+'metrics.npy', np.array([mae, mse, rmse, mape, mspe]))\n",
    "        np.save(folder_path+'pred.npy', preds)\n",
    "        np.save(folder_path+'true.npy', trues)\n",
    "        np.save(folder_path+'label.npy', labels)\n",
    "\n",
    "        return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class STOCK_Hour(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,features='M', \n",
    "                 data_path='General_Electronics_stock_data.csv',target='Close', scale=True):\n",
    "        # size [seq_len, pred_len]\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 900\n",
    "            self.pred_len = 60\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.pred_len = size[1]\n",
    "            \n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train':0, 'val':1, 'test':2}\n",
    "        self.set_type = type_map[flag]\n",
    "        \n",
    "        self.scale = scale\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.target = target\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path))\n",
    "\n",
    "        border1s = [0, 365 - self.seq_len, 365+4*30 - self.seq_len]\n",
    "        border2s = [365, 365+120, 365+240]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "        self.features = 'M' \n",
    "        if self.features=='M':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        \n",
    "        \n",
    "        if self.scale:\n",
    "            data = scaler.fit_transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        \n",
    "            \n",
    "        df_stamp = df_raw[['Date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.Date)\n",
    "        df_stamp['month'] = df_stamp.date.apply(lambda row:row.month,1)\n",
    "        df_stamp['day'] = df_stamp.date.apply(lambda row:row.day,1)\n",
    "        df_stamp['weekday'] = df_stamp.date.apply(lambda row:row.weekday(),1)\n",
    "        df_stamp['hour'] = df_stamp.date.apply(lambda row:row.hour,1)\n",
    "        data_stamp = df_stamp.drop(['date'],1).values\n",
    "        \n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.pred_len\n",
    "        r_end = r_begin + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python main_gta.py --model gta --data General_Electronics_stock_data\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from exp.exp_gta_dad import Exp_GTA_DAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='[GTA] GTA ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--model'], dest='model', nargs=None, const=None, default='gta', type=<class 'str'>, choices=None, help='model of the experiment', metavar=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "parser.add_argument('--model', type=str, required=True, default='gta',help='model of the experiment')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--gpu'], dest='gpu', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help='gpu', metavar=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser.add_argument('--data', type=str, required=True, default='General_Electronics_stock_data', help='data')\n",
    "parser.add_argument('--root_path', type=str, default='./data/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='General_Electronics_stock_data.csv', help='location of the data file')    \n",
    "parser.add_argument('--features', type=str, default='M', help='features [S, M]')\n",
    "parser.add_argument('--target', type=str, default='Close', help='target feature')\n",
    "\n",
    "\n",
    "parser.add_argument('--seq_len', type=int, default=60, help='input series length')\n",
    "#parser.add_argument('--label_len', type=int, default=30, help='help series length')\n",
    "parser.add_argument('--pred_len', type=int, default=24, help='predict series length')\n",
    "parser.add_argument('--num_nodes', type=int, default=7, help='encoder input size')\n",
    "parser.add_argument('--num_levels', type=int, default=3, help='number of dilated levels for graph embedding')\n",
    "# parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "# parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=128, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=2, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=128, help='dimension of fcn')\n",
    "parser.add_argument('--factor', type=int, default=5, help='prob sparse factor')\n",
    "\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--attn', type=str, default='prob', help='attention [prob, full]')\n",
    "parser.add_argument('--embed', type=str, default='fixed', help='embedding type [fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu',help='activation')\n",
    "parser.add_argument('--num_workers', type=int, default=0, help='data loader num workers')\n",
    "\n",
    "parser.add_argument('--itr', type=int, default=2, help='each params run iteration')\n",
    "parser.add_argument('--train_epochs', type=int, default=6, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='input data batch size')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test',help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse',help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1',help='adjust learning rate')\n",
    "\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --model MODEL --data DATA\n",
      "                             [--root_path ROOT_PATH] [--data_path DATA_PATH]\n",
      "                             [--features FEATURES] [--target TARGET]\n",
      "                             [--seq_len SEQ_LEN] [--pred_len PRED_LEN]\n",
      "                             [--num_nodes NUM_NODES] [--num_levels NUM_LEVELS]\n",
      "                             [--d_model D_MODEL] [--n_heads N_HEADS]\n",
      "                             [--e_layers E_LAYERS] [--d_layers D_LAYERS]\n",
      "                             [--d_ff D_FF] [--factor FACTOR]\n",
      "                             [--dropout DROPOUT] [--attn ATTN] [--embed EMBED]\n",
      "                             [--activation ACTIVATION]\n",
      "                             [--num_workers NUM_WORKERS] [--itr ITR]\n",
      "                             [--train_epochs TRAIN_EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--patience PATIENCE]\n",
      "                             [--learning_rate LEARNING_RATE] [--des DES]\n",
      "                             [--loss LOSS] [--lradj LRADJ] [--use_gpu USE_GPU]\n",
      "                             [--gpu GPU]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --model, --data\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main_gta.py --model gta --data General_Electronics_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m data_parser \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mGeneral_Electronics_stock_data\u001b[39m\u001b[39m'\u001b[39m:{\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mGeneral_Electronics_stock_data.csv\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mT\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mM\u001b[39m\u001b[39m'\u001b[39m:[\u001b[39m7\u001b[39m,\u001b[39m7\u001b[39m,\u001b[39m7\u001b[39m],\u001b[39m'\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m:[\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m]}}\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39mGTA\n\u001b[1;32m      6\u001b[0m \u001b[39m# ---------------------------- Need to change according to our data ------------------------------\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39min\u001b[39;00m data_parser\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "data_parser = {\n",
    "    'General_Electronics_stock_data':{'data':'General_Electronics_stock_data.csv','T':'Close','M':[7,7,7],'S':[1,1,1]}}\n",
    "\n",
    "model = self.GTA\n",
    "\n",
    "# ---------------------------- Need to change according to our data ------------------------------\n",
    "if data in data_parser.keys():\n",
    "    data_info = data_parser[data]\n",
    "    data_path = data_info['data']\n",
    "    target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "Exp = Exp_GTA_DAD\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    # ---------------------------- Need to change according to our data ------------------------------\n",
    "    setting = '{}_{}_ft{}_sl{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_eb{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len,  args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.embed, args.des, ii)\n",
    "    # ------------------------------------------------------------------------------------------------\n",
    "    exp = Exp(args)\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "    \n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
